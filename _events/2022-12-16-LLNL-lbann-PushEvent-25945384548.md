---
event_type: PushEvent
avatar: "https://avatars.githubusercontent.com/u/6210171?"
user: bvanessen
date: 2022-12-16
repo_name: LLNL/lbann
html_url: https://github.com/LLNL/lbann/commit/9ca712cc3d38e010e54354a3013414c029c2aa85
repo_url: https://github.com/LLNL/lbann
---

<a href='https://github.com/bvanessen' target='_blank'>bvanessen</a> pushed to <a href='https://github.com/LLNL/lbann' target='_blank'>LLNL/lbann</a>

<small>Bugfix input layer activations (#2164)

* Fixed the input layer so that it would only resize the activation
matrix if it wasn't already setup to be a view of the
data_coordinator's matrix.  This addresses a signficant performance
bug in the data ingestion where the activation matrix was a view into
the data coordinator's internal buffers.  Then on each step the matrix
would be deallocated, reallocated, then set to a view again.  This
also triggered a performance problem in the memory pool allocator in
Hydrogen, which is not designed to cache very large allocations.

Added guards in the top level data_type_layer to make sure that
matrices that are views of other matrices are not resized.

* Added a release note about the performance fix

* Updated the distconv identity test to force it to keep its error
signals to avoid accessing non-existent previous error signals.</small>

<a href='https://github.com/LLNL/lbann/commit/9ca712cc3d38e010e54354a3013414c029c2aa85' target='_blank'>View Commit</a>