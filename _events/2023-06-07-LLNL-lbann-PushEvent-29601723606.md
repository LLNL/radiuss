---
event_type: PushEvent
avatar: "https://avatars.githubusercontent.com/u/6210171?"
user: bvanessen
date: 2023-06-07
repo_name: LLNL/lbann
html_url: https://github.com/LLNL/lbann/commit/868e098dfc2e4a97aac2e3330d2d384156412180
repo_url: https://github.com/LLNL/lbann
---

<a href='https://github.com/bvanessen' target='_blank'>bvanessen</a> pushed to <a href='https://github.com/LLNL/lbann' target='_blank'>LLNL/lbann</a>

<small>Transformer optimizations (#2269)

* Add multi-head attention unit test and fix transformer module

* Update dataset path

* Fix weighted sum operation and add test

* Make weighted sum in-place-capable, ensure backprop runs all the way through in testing

* Enable softmax single-dimension mode for ChannelwiseSoftmax. This is used in the batched-head Transformer implementation

* Transformer optimizations: stacked QKV multiplication and batched heads

* Update miscellaneous_layers.rst

* Apply suggestion from review</small>

<a href='https://github.com/LLNL/lbann/commit/868e098dfc2e4a97aac2e3330d2d384156412180' target='_blank'>View Commit</a>