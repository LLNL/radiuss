---
event_type: PushEvent
avatar: "https://avatars.githubusercontent.com/u/6210171?"
user: bvanessen
date: 2024-02-14
repo_name: LLNL/lbann
html_url: https://github.com/LLNL/lbann/commit/84b8393154105474cbcd0e13fc8a6b2f5728f366
repo_url: https://github.com/LLNL/lbann
---

<a href='https://github.com/bvanessen' target='_blank'>bvanessen</a> pushed to <a href='https://github.com/LLNL/lbann' target='_blank'>LLNL/lbann</a>

<small>Transformer extensions and refactoring (#2411)

* Refactor positional encoding into its own file, extend its capabilities
* Add potential attention bias to attention matrix in multi-head attention
* Allow users to modify the multi-head attention module
* Implement Rotary Position Embedding (RoPE)
* Add multi-dimensional support to layer normalization</small>

<a href='https://github.com/LLNL/lbann/commit/84b8393154105474cbcd0e13fc8a6b2f5728f366' target='_blank'>View Commit</a>